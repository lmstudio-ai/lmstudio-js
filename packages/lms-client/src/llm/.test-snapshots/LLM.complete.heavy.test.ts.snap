// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`LLM.complete should call onPredictionFragment 1`] = `
[
  [
    {
      "containsDrafted": false,
      "content": "4",
      "isStructural": false,
      "reasoningType": "none",
      "tokensCount": 1,
    },
  ],
  [
    {
      "containsDrafted": false,
      "content": ";",
      "isStructural": false,
      "reasoningType": "none",
      "tokensCount": 1,
    },
  ],
]
`;

exports[`LLM.complete should support reasoning content parsing 4`] = `
[
  {
    "content": "4",
    "reasoningType": "none",
  },
  {
    "content": ",",
    "reasoningType": "none",
  },
  {
    "content": "5",
    "reasoningType": "reasoningStartTag",
  },
  {
    "content": ",",
    "reasoningType": "reasoning",
  },
  {
    "content": "6",
    "reasoningType": "reasoning",
  },
  {
    "content": ",",
    "reasoningType": "reasoning",
  },
  {
    "content": "7",
    "reasoningType": "reasoningEndTag",
  },
  {
    "content": ",",
    "reasoningType": "none",
  },
  {
    "content": "8",
    "reasoningType": "none",
  },
  {
    "content": ",",
    "reasoningType": "none",
  },
]
`;

exports[`LLM.complete should support structured generation with GBNF grammar 1`] = `"Oh no, I am possessed! And 1 + 1 is 2"`;

exports[`LLM.complete should support structured prediction with JSON schema 1`] = `
{
  "answer": 2,
}
`;

exports[`LLM.complete should support structured prediction with zod schema 1`] = `
{
  "answer": 2,
}
`;

exports[`LLM.complete should support structured prediction with zod schema 2`] = `
{
  "answer": 2,
}
`;

exports[`LLM.complete should work with streaming 1`] = `
[
  {
    "containsDrafted": false,
    "content": "4",
    "isStructural": false,
    "reasoningType": "none",
    "tokensCount": 1,
  },
]
`;

exports[`LLM.complete should work with streaming 2`] = `
{
  "numGpuLayers": Any<Number>,
  "predictedTokensCount": 1,
  "promptTokensCount": 15,
  "stopReason": "stopStringFound",
  "timeToFirstTokenSec": Any<Number>,
  "tokensPerSecond": Any<Number>,
  "totalTimeSec": Any<Number>,
  "totalTokensCount": 16,
}
`;

exports[`LLM.complete should work with streaming 3`] = `
{
  "architecture": "qwen2",
  "contextLength": 4096,
  "displayName": "Qwen2.5 0.5B Instruct",
  "format": "gguf",
  "identifier": Any<String>,
  "instanceReference": Any<String>,
  "maxContextLength": 32768,
  "modelKey": Any<String>,
  "paramsString": "0.5B",
  "path": "lmstudio-community/Qwen2.5-0.5B-Instruct-GGUF/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf",
  "quantization": {
    "bits": 4,
    "name": "Q4_K_M",
  },
  "sizeBytes": 397807936,
  "trainedForToolUse": true,
  "type": "llm",
  "vision": false,
}
`;

exports[`LLM.complete should work without streaming 2`] = `
{
  "numGpuLayers": Any<Number>,
  "predictedTokensCount": 1,
  "promptTokensCount": 15,
  "stopReason": "stopStringFound",
  "timeToFirstTokenSec": Any<Number>,
  "tokensPerSecond": Any<Number>,
  "totalTimeSec": Any<Number>,
  "totalTokensCount": 16,
}
`;

exports[`LLM.complete should work without streaming 3`] = `
{
  "architecture": "qwen2",
  "contextLength": 4096,
  "displayName": "Qwen2.5 0.5B Instruct",
  "format": "gguf",
  "identifier": Any<String>,
  "instanceReference": Any<String>,
  "maxContextLength": 32768,
  "modelKey": Any<String>,
  "paramsString": "0.5B",
  "path": "lmstudio-community/Qwen2.5-0.5B-Instruct-GGUF/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf",
  "quantization": {
    "bits": 4,
    "name": "Q4_K_M",
  },
  "sizeBytes": 397807936,
  "trainedForToolUse": true,
  "type": "llm",
  "vision": false,
}
`;
